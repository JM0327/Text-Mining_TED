---
title: "sentiment"
author: "Jiaming"
date: "2022-12-10"
output: html_document
---

```{r}
source(here::here("scripts/setup.R"))
library(flextable)

```


use three dictionary:AFINN,Bing,NRC

https://www.datacamp.com/tutorial/sentiment-analysis-R:
The tidytext package includes a dataset called sentiments which provides several distinct lexicons. These lexicons are dictionaries of words with an assigned sentiment category or value. tidytext provides three general purpose lexicons:

AFINN: assigns words with a score that runs between -5 and 5, with negative scores indicating negative sentiment and positive scores indicating positive sentiment  
Bing: assigns words into positive and negative categories  
NRC: assigns words into one or more of the following ten categories: positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust   

```{r}
# Import and clean data
TED <- read_csv(here::here("data/TED.csv"))
views_add <- read_csv(here::here("data/add_details_1.csv"))

TED <- left_join(TED,views_add, by = c("title"="page_title")) # add views in each video page
TED <- TED %>% filter(is.na(TED$tanscript)==F)  # drop the rows having na caused by left join 
TED <- TED %>% filter(is.na(TED$views_details)==F)


# extract the number of views times 
# first separate the views detail into two parts (before "views" after "views")
views_time <- as.character()
for (i in 1:length(TED$views_details)) {
  views_temp <- TED$views_details[i]
  views_temp <- strsplit(views_temp, "views")
  views_temp <- views_temp[[1]][1]
  views_time <- append(views_time,views_temp)
}
views_time <- gsub(" ","",views_time)
views_time <- gsub(",","",views_time)
TED$views_details <- as.numeric(views_time)
```

```{r}
TED_sentiment <- TED %>% select(title,speaker.x,views_times.x,cate,likes,tanscript,views_details)

TED_sentiment <- na.omit(TED_sentiment)
colnames(TED_sentiment)[2] <- "speaker"
colnames(TED_sentiment)[3] <- "posted"


TED_sentiment$likes <- gsub("[(]",'',TED_sentiment$likes)
TED_sentiment$likes <- gsub("[)]",'',TED_sentiment$likes)
x <- substr(TED_sentiment$likes[1],1,1)
TED_sentiment$likes <- gsub(x,'',TED_sentiment$likes)
TED_sentiment$likes <- gsub("K", "e3", TED_sentiment$likes)
TED_sentiment$likes <- gsub("M", "e6", TED_sentiment$likes)
TED_sentiment$likes <- as.numeric(TED_sentiment$likes)

# Clean transcript

TED_sentiment$tanscript <- gsub("^.+?00:(.*)","\\1",TED_sentiment$tanscript)
TED_sentiment$tanscript <- gsub("\r\n"," ",TED_sentiment$tanscript)
TED_sentiment$tanscript <- gsub("[[:digit:]]"," ",TED_sentiment$tanscript)
```


```{r}
# since sentiment analysis cannot use cleaned data after stemming, so here use another way to tokenize again

TED.tok <- unnest_tokens(
  TED_sentiment,
  output = "word",
  input = "tanscript",
  to_lower = TRUE,
  strip_punct = TRUE,
  strip_numeric = TRUE)
TED.tok <- TED.tok %>% filter(word != "laughter" | word != "applaud")
```

```{r}
#NRC
#get_sentiments(lexicon = "nrc")
TED.sent.nrc <- 
  inner_join(
    TED.tok,
    get_sentiments("nrc"),
    by = c("word" = "word"))
head(TED.sent.nrc, 10) %>% flextable() %>% autofit()

# TED.sent.bing <- 
#   inner_join(
#     TED.tok,
#     get_sentiments("bing"),
#     by = c("word" = "word"))
# head(TED.sent.nrc, 10) %>% flextable() %>% autofit()

# Afinn
TED.sent.afinn <- 
  inner_join(
    TED.tok,
    get_sentiments("afinn"),
    by = c("word" = "word"))

TED.sent.afinn %>% 
  group_by(title) %>% 
  summarize(Score = mean(value)) %>% 
  ungroup() %>% 
  arrange(Score) %>% 
  flextable() %>% 
  autofit()
```


Sentiment Based

```{r}
# video likes

TED.nrc.like <- TED.sent.nrc %>% 
  select(title,word,likes,sentiment) %>%
  group_by(title,likes,sentiment) %>% summarise(n=n())

# TED.bing.like <- TED.sent.bing %>% 
#   select(title,word,likes,sentiment) %>%
#   group_by(title,likes,sentiment) %>% summarise(n=n())

toplike20 <- TED.nrc.like[order(TED.nrc.like$likes,decreasing = T),][1:200,]
taillike20 <- TED.nrc.like[order(TED.nrc.like$likes,decreasing = F),][1:200,]

toplike20%>%
  ggplot(mapping = aes(x = sentiment, y=n, fill = sentiment)) + 
  geom_bar(stat = "identity",
           alpha = 0.8) + 
  facet_wrap(~ title) + 
  coord_flip()

taillike20%>%
  ggplot(mapping = aes(x = sentiment, y=n, fill = sentiment)) + 
  geom_bar(stat = "identity",
           alpha = 0.8) + 
  facet_wrap(~ title) + 
  coord_flip()

# TED.bing.like[order(TED.bing.like$likes,decreasing = T),][1:20,] %>%
#   ggplot(mapping = aes(x = sentiment, y=n, fill = sentiment)) + 
#   geom_bar(stat = "identity",
#            alpha = 0.8) + 
#   facet_wrap(~ title) + 
#   coord_flip()
  
```
rescale them by their length (i.e. the frequencies of sentiments are computed, by document):

```{r}
TED.sent.nrc.total <- 
  TED.sent.nrc %>% 
  group_by(title,likes) %>% 
  summarize(Total = n()) %>% 
  ungroup()


left_join(
  TED.sent.nrc,
  TED.sent.nrc.total
)%>% 
  filter(title %in% toplike20$title) %>%
  group_by(title, sentiment) %>%  
  summarize(n = n(),
            Total = unique(Total)) %>%
  ungroup() %>% 
  mutate(relfreq = n / Total) %>%
  ggplot(aes(
    x = sentiment,
    y = relfreq,
    fill = sentiment)) + 
  geom_bar(stat = "identity", alpha = 0.8) + 
  facet_wrap(~ title) + 
  coord_flip()

left_join(
  TED.sent.nrc,
  TED.sent.nrc.total
)%>% 
  filter(title %in% taillike20$title) %>%
  group_by(title, sentiment) %>%  
  summarize(n = n(),
            Total = unique(Total)) %>%
  ungroup() %>% 
  mutate(relfreq = n / Total) %>%
  ggplot(aes(
    x = sentiment,
    y = relfreq,
    fill = sentiment)) + 
  geom_bar(stat = "identity", alpha = 0.8) + 
  facet_wrap(~ title) + 
  coord_flip()
```

Value-Based

```{r}

```



