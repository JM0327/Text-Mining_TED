---
title: "Tokenization"
author: "Manunpat"
date: "2022-12-03"
output: html_document
---

```{r,include=FALSE}
source(here::here("scripts/setup.R"))
```

# III Tokenization
## Wrangling and parsing data

After scraping data from TED.com website, we imported two tables saved in .csv format in the data folder, namely TED.csv and add_details_1.csv. The original tables consist of 330 observations with 11 variables and 310 observations with 2 variables, respectively. 
```{r, include=FALSE}
# Import data
TED <- read_csv(here::here("data/TED.csv")) #330 obs
views_add <- read_csv(here::here("data/add_details_1.csv")) #310 obs

kable(TED[1:10,], caption = "The original TED table") %>%
   kable_styling(bootstrap_options = "bordered") %>%
   kableExtra::scroll_box(width = "100%", height = "250px")

kable(TED[1:10,], caption = "The original add_details_1 table") %>%
   kable_styling(bootstrap_options = "bordered") %>%
   kableExtra::scroll_box(width = "100%", height = "250px")
```

We then removed duplicated observations for both tables and combined two tables by title column and named it as **TED**. TED table currently contains 324 observations with 12 variables. However, we would like to focus just 6 interesting variables, which were title of videos, when the videos were posted, topic of videos, the number of likes for videos, transcript, and the number of views of videos, for our further analyses, so we selected them and removed the rest. We also spotted 34 NAs in **TED** and they were removed later. Therefore, **TED** have 286 observations which are 103 videos from AI, 86 videos from Climate change, and 97 videos from Relationships.
```{r}
# Delete duplicate rows of TED
sum(duplicated(TED)) #6
TED <- TED[!duplicated(TED), ]

# Delete duplicate rows of views_add
unique(views_add$page_title) #304 so duplicate title = 6
views_add <- views_add[!duplicated(views_add), ] 

# Combine tables
TED <- left_join(TED,views_add, by = c("title"="page_title")) 
TED <- TED %>% as_tibble(data.frame(TED)) %>%
  select(title, views_times.x, cate, likes, tanscript, views_details)
colnames(TED)[2] <- "posted"

# Identify NA and remove
# checkna <- TED[is.na(TED$tanscript), ]
# numtotal <- data.frame(table(TED$cate))
# numna <- data.frame(table(checkna$cate))
# diff <- numtotal %>% left_join(numna, by = "Var1") %>%
#   mutate(diff = Freq.x-Freq.y)
TED <- na.omit(TED) #286
catenum <- data.frame(table(TED$cate))
colnames(catenum) <- c("Topics", "Count")

kable(catenum[,], caption = "") %>%
   kable_styling(bootstrap_options = "bordered") %>%
   kableExtra::scroll_box(width = "100%", height = "250px")
```

Subsequently, it turned to data parsing step. We converted posting time, the number of likes, the number of views to be in the appropriate format for further analyses. For example, the posting time for the first video, *"How does artificial intelligence learn?"*, was Mar 2021 in the original TED table. It was converted to be 2021-03-01. 

For the transcript, there were the number of translated languages and the details of the translation at the beginning of the transcript every video. In this project, we focus only the actual transcript. Thus, we removed this part out of the transcript. For example, the first sentence of the transcript in the first video,*"How does artificial intelligence learn?"*, was "Transcript (28 Languages)Bahasa IndonesiaDeutschEnglishEspañolFrançaisItalianoMagyarPolskiPortuguês brasileiroPortuguês de PortugalRomânăTiếng ViệtTürkçeΕλληνικάРусскийСрпски, Srpskiעבריתالعربيةفارسىکوردی سۆرانیবাংলাதமிழ்ภาษาไทยမြန်မာဘာသာ中文 (简体)中文 (繁體)日本語한국어". We removed this part out of the transcript.

```{r}
# Parse data
TED$posted <- my(TED$posted)
TED$cate[which(TED$cate=="AI")] <- "1"
TED$cate[which(TED$cate=="Climate change")] <- "2"
TED$cate[which(TED$cate=="Relationships")] <- "3"
TED$likes <- gsub("[(]",'',TED$likes)
TED$likes <- gsub("[)]",'',TED$likes)
x <- substr(TED$likes[1],1,1)
TED$likes <- gsub(x,'',TED$likes)
TED$likes <- gsub("K", "e3", TED$likes)
TED$likes <- gsub("M", "e6", TED$likes)
TED$likes <- as.numeric(TED$likes)

# Clean number of views
# first separate the views detail into two parts (before "views" after "views")
views_time <- as.character()
for (i in 1:length(TED$views_details)) {
  views_temp <- TED$views_details[i]
  views_temp <- strsplit(views_temp, "views")
  views_temp <- views_temp[[1]][1]
  views_time <- append(views_time,views_temp)
}
views_time <- gsub(" ","",views_time)
views_time <- gsub(",","",views_time)
TED$views_details <- as.numeric(views_time)

# Clean transcript
TED$tanscript <- gsub("^.+?00:(.*)","\\1",TED$tanscript)
TED$tanscript <- gsub("\r\n"," ",TED$tanscript)
TED$tanscript <- gsub("[[:digit:]]"," ",TED$tanscript)

# extract on version of TED for sentiment part
TED_sentiment <- TED
# no need for title column in the following analysis
TED <- TED %>% select(-title)
```

Due to the limited number of available videos within the selected topics on TED website, we could not scrape more videos for unsupervised and supervised learning analyses, and we would like to obtain a robust model as well as avoid the overfitting problem. Therefore, we decided to increase the number of our observations by setting up a window of 20 sentences to be equal to 1 observation as we noticed that the transcripts each video comprised of more than 20 sentences. 

We also converted the topic names, AI, Climate change, and Relationships, to be numbers 1, 2, and 3. This would be easy to keep track of the videos in supervised and unsupervised learning analyses.

We split sentences by using the token...sentence function from ... package. Then add sub cate. For example, sub_cat of 1.2 indicates that the observation is from the 2nd transcript in Topic 1. Afterwards we add text column to indicate and identifier of each text. For ex, a text id of 1.2.3 indicate that this text comes from the third sentence of the second transcript in topic 1(AI)

```{r}
# Increase the number of instances: 20 sentences = 1 instance
TED_full <- TED[0,]
TED_full$subcate <- TED_full$cate #new col but same type
TED_full$text <- TED_full$cate
n_transcript <- length(TED$tanscript)

sub_cate_1 = 0
sub_cate_2 = 0
sub_cate_3 = 0

for (i in 1:n_transcript) {
  
  if (TED$cate[i] == "1") {
    sub_cate_1 <- sub_cate_1 + 1
    subcat_temp =  paste(TED$cate[i],".",as.character(sub_cate_1), sep = "", collapse = "")
  } 
  else if (TED$cate[i] == "2") {
    sub_cate_2 <- sub_cate_2 + 1
    subcat_temp =  paste(TED$cate[i],".",as.character(sub_cate_2), sep = "", collapse = "")
  }
  else {
    sub_cate_3 <- sub_cate_3 + 1
    subcat_temp =  paste(TED$cate[i],".",as.character(sub_cate_3), sep = "", collapse = "")
  }
   
  transcript_i <- TED$tanscript[i]
  transcript_i_sentence <- unlist(tokenize_sentence(transcript_i))
  n_sen <- length(transcript_i_sentence)
  n_group <- ceiling(n_sen/20)
  for (j in 1:n_group) {
    if (j == n_group) {
      sentence_temp <- paste(transcript_i_sentence[((j-1)*20+1):(n_sen)], collapse = " ")
    } 
    else {
      sentence_temp <- paste(transcript_i_sentence[((j-1)*20+1):(j*20)], collapse = " ")
    }
    
    text_temp = paste(subcat_temp,".",as.character(j), sep = "", collapse = "")
    TED_temp <- data.frame(posted = TED$posted[i], cate = TED$cate[i], like = TED$likes[i], view = TED$views_details[i], subcate = subcat_temp, text = text_temp, tanscript = sentence_temp)
    TED_full <- rbind(TED_full, TED_temp)
  }
    
}

TED_full$tanscript <- trim_ws(TED_full$tanscript)

# Our final table = TED_full consisting of 1471 instances

# kable(TED[, ], caption = "The original TED table") %>%
#   kable_styling(bootstrap_options = "bordered") %>%
#   kableExtra::scroll_box(width = "100%", height = "250px")  

```

# Tokenization from TED_full for unsupervised and supervised analyses

I aim to create DTM, TFIDF tables by quanteda package. However, I also token TED_full by tidytext as well 
```{r}
# Quanteda
TED.cp <- corpus(TED_full$tanscript)
summary(TED.cp)

TED.tk <- tokens(
  TED.cp, 
  remove_numbers = TRUE, 
  remove_punct = TRUE, 
  remove_symbols = TRUE, 
  remove_separators = TRUE)

TED.tk <- TED.tk %>% 
  tokens_tolower() %>% 
  tokens_remove(c(stopwords(source = "smart"), "applaud", "laughter"))

TED.tk <- tokens_replace(
  TED.tk,
  pattern = hash_lemmas$token, 
  replacement = hash_lemmas$lemma)

TED.tk

```

## DTM

```{r}
TED.dfm <- dfm(TED.tk)
head(TED.dfm)
```

```{r}
TED.freq <- textstat_frequency(TED.dfm)
head(TED.freq, 20)
```

## TF-IDF

```{r}
# Quenteda
TED.tfidf <- dfm_tfidf(TED.dfm)  
sort(apply(TED.tfidf, 2, max), decreasing = TRUE)[1:10]

```

# Tokenization from TED for EDA

```{r}
# Quanteda
TED.cp1 <- corpus(TED$tanscript)
summary(TED.cp1)

TED.tk1 <- tokens(
  TED.cp1, 
  remove_numbers = TRUE, 
  remove_punct = TRUE, 
  remove_symbols = TRUE, 
  remove_separators = TRUE)

TED.tk1 <- TED.tk1 %>% 
  tokens_tolower() %>% 
  tokens_remove(c(stopwords(source = "smart"), "applaud", "laughter"))

TED.tk1 <- tokens_replace(
  TED.tk1,
  pattern = hash_lemmas$token, 
  replacement = hash_lemmas$lemma)

TED.tk1


TED.dfm1 <- dfm(TED.tk1)
head(TED.dfm1)
TED.freq1 <- textstat_frequency(TED.dfm1)
head(TED.freq1, 20)
TED.tfidf1 <- dfm_tfidf(TED.dfm1)  
sort(apply(TED.tfidf1, 2, max), decreasing = TRUE)[1:10]

```



