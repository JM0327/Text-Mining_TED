---
title: "Tokenization"
author: "Manunpat"
date: "2022-12-03"
output: html_document
---

```{r,include=FALSE}
source(here::here("scripts/setup.R"))
```

# Cleanning and parsing data
....
Unfortunately, we found 34 videos in total do not have information in transcript. One video is from AI while 22 videos and 11 videos are from Climate change and Relationships categories, respectively. 
....
```{r}
# Import data
TED <- read_csv(here::here("data/TED.csv")) #330 obs
views_add <- read_csv(here::here("data/add_details_1.csv")) #310 obs
```

```{r}
# Delete duplicate rows of TED
sum(duplicated(TED)) #6
TED <- TED[!duplicated(TED), ]

# Delete duplicate rows of views_add
unique(views_add$page_title) #304 so duplicate title = 6
views_add <- views_add[!duplicated(views_add), ] 

# Combine tables
TED <- left_join(TED,views_add, by = c("title"="page_title")) 
TED <- TED %>% as_tibble(data.frame(TED)) %>%
  select(views_times.x, cate, likes, tanscript, views_details)
colnames(TED)[1] <- "posted"

# Identify NA and remove
# checkna <- TED[is.na(TED$tanscript), ]
# numtotal <- data.frame(table(TED$cate))
# numna <- data.frame(table(checkna$cate))
# diff <- numtotal %>% left_join(numna, by = "Var1") %>%
#   mutate(diff = Freq.x-Freq.y)
TED <- na.omit(TED) #286
catenum <- data.frame(table(TED$cate))

# Parse data
TED$posted <- my(TED$posted)
TED$cate[which(TED$cate=="AI")] <- "1"
TED$cate[which(TED$cate=="Climate change")] <- "2"
TED$cate[which(TED$cate=="Relationships")] <- "3"
TED$likes <- gsub("[(]",'',TED$likes)
TED$likes <- gsub("[)]",'',TED$likes)
x <- substr(TED$likes[1],1,1)
TED$likes <- gsub(x,'',TED$likes)
TED$likes <- gsub("K", "e3", TED$likes)
TED$likes <- gsub("M", "e6", TED$likes)
TED$likes <- as.numeric(TED$likes)

# Clean number of views
# first separate the views detail into two parts (before "views" after "views")
views_time <- as.character()
for (i in 1:length(TED$views_details)) {
  views_temp <- TED$views_details[i]
  views_temp <- strsplit(views_temp, "views")
  views_temp <- views_temp[[1]][1]
  views_time <- append(views_time,views_temp)
}
views_time <- gsub(" ","",views_time)
views_time <- gsub(",","",views_time)
TED$views_details <- as.numeric(views_time)

# Clean transcript
TED$tanscript <- gsub("^.+?00:(.*)","\\1",TED$tanscript)
TED$tanscript <- gsub("\r\n"," ",TED$tanscript)
TED$tanscript <- gsub("[[:digit:]]"," ",TED$tanscript)

# Increase the number of instances: 20 sentences = 1 instance
TED_full <- TED[0,]
TED_full$subcate <- TED_full$cate #new col but same type
TED_full$text <- TED_full$cate
n_transcript <- length(TED$tanscript)

sub_cate_1 = 0
sub_cate_2 = 0
sub_cate_3 = 0

for (i in 1:n_transcript) {
  
  if (TED$cate[i] == "1") {
    sub_cate_1 <- sub_cate_1 + 1
    subcat_temp =  paste(TED$cate[i],".",as.character(sub_cate_1), sep = "", collapse = "")
  } 
  else if (TED$cate[i] == "2") {
    sub_cate_2 <- sub_cate_2 + 1
    subcat_temp =  paste(TED$cate[i],".",as.character(sub_cate_2), sep = "", collapse = "")
  }
  else {
    sub_cate_3 <- sub_cate_3 + 1
    subcat_temp =  paste(TED$cate[i],".",as.character(sub_cate_3), sep = "", collapse = "")
  }
   
  transcript_i <- TED$tanscript[i]
  transcript_i_sentence <- unlist(tokenize_sentence(transcript_i))
  n_sen <- length(transcript_i_sentence)
  n_group <- ceiling(n_sen/20)
  for (j in 1:n_group) {
    if (j == n_group) {
      sentence_temp <- paste(transcript_i_sentence[((j-1)*20+1):(n_sen)], collapse = " ")
    } 
    else {
      sentence_temp <- paste(transcript_i_sentence[((j-1)*20+1):(j*20)], collapse = " ")
    }
    
    text_temp = paste(subcat_temp,".",as.character(j), sep = "", collapse = "")
    TED_temp <- data.frame(posted = TED$posted[i], cate = TED$cate[i], like = TED$likes[i], view = TED$views_details[i], subcate = subcat_temp, text = text_temp, tanscript = sentence_temp)
    TED_full <- rbind(TED_full, TED_temp)
  }
    
}

TED_full$tanscript <- trim_ws(TED_full$tanscript)

# Our final table = TED_full consisting of 1471 instances

# kable(TED[, ], caption = "The original TED table") %>%
#   kable_styling(bootstrap_options = "bordered") %>%
#   kableExtra::scroll_box(width = "100%", height = "250px")  

```

# Tokenization from TED_full for unsupervised and supervised analyses

I aim to create DTM, TFIDF tables by quanteda package. However, I also token TED_full by tidytext as well 
```{r}
# Quanteda
TED.cp <- corpus(TED_full$tanscript)
summary(TED.cp)

TED.tk <- tokens(
  TED.cp, 
  remove_numbers = TRUE, 
  remove_punct = TRUE, 
  remove_symbols = TRUE, 
  remove_separators = TRUE)

TED.tk <- TED.tk %>% 
  tokens_tolower() %>% 
  tokens_remove(c(stopwords(source = "smart"), "applaud", "laughter"))

TED.tk <- tokens_replace(
  TED.tk,
  pattern = hash_lemmas$token, 
  replacement = hash_lemmas$lemma)

TED.tk

```

## DTM

```{r}
TED.dfm <- dfm(TED.tk)
head(TED.dfm)
```

```{r}
TED.freq <- textstat_frequency(TED.dfm)
head(TED.freq, 20)
```

## TF-IDF

```{r}
# Quenteda
TED.tfidf <- dfm_tfidf(TED.dfm)  
sort(apply(TED.tfidf, 2, max), decreasing = TRUE)[1:10]

```

# Tokenization from TED for EDA

```{r}
# Quanteda
TED.cp1 <- corpus(TED$tanscript)
summary(TED.cp1)

TED.tk1 <- tokens(
  TED.cp1, 
  remove_numbers = TRUE, 
  remove_punct = TRUE, 
  remove_symbols = TRUE, 
  remove_separators = TRUE)

TED.tk1 <- TED.tk1 %>% 
  tokens_tolower() %>% 
  tokens_remove(c(stopwords(source = "smart"), "applaud", "laughter"))

TED.tk1 <- tokens_replace(
  TED.tk1,
  pattern = hash_lemmas$token, 
  replacement = hash_lemmas$lemma)

TED.tk1


TED.dfm1 <- dfm(TED.tk1)
head(TED.dfm1)
TED.freq1 <- textstat_frequency(TED.dfm1)
head(TED.freq1, 20)
TED.tfidf1 <- dfm_tfidf(TED.dfm1)  
sort(apply(TED.tfidf1, 2, max), decreasing = TRUE)[1:10]

```



