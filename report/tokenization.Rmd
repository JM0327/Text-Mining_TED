---
title: "Tokenization"
author: "Manunpat"
date: "2022-12-03"
output: html_document
---

```{r,include=FALSE}
source(here::here("scripts/setup.R"))
```

# 3. Tokenization
## Wrangling and parsing data

After scraping data from TED.com website, we imported two tables saved in .csv format in the data folder, namely TED.csv and add_details_1.csv. The original tables consist of 330 observations with 11 variables and 310 observations with 2 variables, respectively. 
```{r, echo=TRUE, warning=FALSE}
# Import data
TED <- read_csv(here::here("data/TED.csv")) #330 obs
views_add <- read_csv(here::here("data/add_details_1.csv")) #310 obs

kable(TED[1:10,], caption = "The example of original TED table") %>%
   kable_styling(bootstrap_options = "bordered") %>%
   kableExtra::scroll_box(width = "100%", height = "250px")

kable(TED[1:10,], caption = "The example of original add_details_1 table") %>%
   kable_styling(bootstrap_options = "bordered") %>%
   kableExtra::scroll_box(width = "100%", height = "250px")
```

We then removed duplicated observations for both tables and combined two tables by title column and named it as **TED**. TED table currently contains 324 observations with 12 variables. However, we would like to focus just 6 interesting variables, which were title of videos (title), when the videos were posted (posted), topic of videos (cate), the number of likes for videos (likes), transcript (tanscript), and the number of views of videos (views_details), for our further analyses, so we selected them and removed the rest. The title variable would be employed for only a sentiment analysis. Hence, we stored all 6 variables in **TED_sentiment**, which performs the main table of the sentiment analysis, and removed the title variable from **TED**.

We also spotted 34 missing values (NAs) in **TED** and they were removed later. Therefore, **TED** have 286 observations which are 103 videos from AI, 86 videos from Climate change, and 97 videos from Relationships.
```{r, echo=TRUE, warning=FALSE}
# Delete duplicate rows of TED
sum(duplicated(TED)) #6
TED <- TED[!duplicated(TED), ]

# Delete duplicate rows of views_add
#unique(views_add$page_title) #304 so duplicate title = 6
views_add <- views_add[!duplicated(views_add), ] 

# Combine tables
TED <- left_join(TED,views_add, by = c("title"="page_title")) 
TED <- TED %>% as_tibble(data.frame(TED)) %>%
  select(title, views_times.x, cate, likes, tanscript, views_details)
colnames(TED)[2] <- "posted"

# Identify NA and remove
# checkna <- TED[is.na(TED$tanscript), ]
# numtotal <- data.frame(table(TED$cate))
# numna <- data.frame(table(checkna$cate))
# diff <- numtotal %>% left_join(numna, by = "Var1") %>%
#   mutate(diff = Freq.x-Freq.y)
TED <- na.omit(TED) #286
catenum <- data.frame(table(TED$cate))
colnames(catenum) <- c("Topics", "Count")

kable(catenum[,], caption = "The number of videos per topics") %>%
   kable_styling(bootstrap_options = "bordered") %>%
   kableExtra::scroll_box(width = "100%", height = "250px")
```

Subsequently, it turned to data parsing step. We converted posting time, the number of likes, the number of views to be in the appropriate format for further analyses. For example, the posting time for the first video, *"How does artificial intelligence learn?"*, was Mar 2021 in the original TED table. It was converted to be 2021-03-01. 

For the transcript, there were the number of translated languages and the details of the translation at the beginning of the transcript every video. In this project, we focus only the actual transcript. Thus, we removed this part out of the transcript. For example, the first sentence of the transcript in the first video,*"How does artificial intelligence learn?"*, was "Transcript (28 Languages)Bahasa IndonesiaDeutschEnglishEspañolFrançaisItalianoMagyarPolskiPortuguês brasileiroPortuguês de PortugalRomânăTiếng ViệtTürkçeΕλληνικάРусскийСрпски, Srpskiעבריתالعربيةفارسىکوردی سۆرانیবাংলাதமிழ்ภาษาไทยမြန်မာဘာသာ中文 (简体)中文 (繁體)日本語한국어". We removed this part out of the transcript.

```{r, echo=TRUE, warning=FALSE}
# Parse data
TED$posted <- my(TED$posted)
TED$cate[which(TED$cate=="AI")] <- "1"
TED$cate[which(TED$cate=="Climate change")] <- "2"
TED$cate[which(TED$cate=="Relationships")] <- "3"
TED$likes <- gsub("[(]",'',TED$likes)
TED$likes <- gsub("[)]",'',TED$likes)
x <- substr(TED$likes[1],1,1)
TED$likes <- gsub(x,'',TED$likes)
TED$likes <- gsub("K", "e3", TED$likes)
TED$likes <- gsub("M", "e6", TED$likes)
TED$likes <- as.numeric(TED$likes)

# Clean number of views
# first separate the views detail into two parts (before "views" after "views")
views_time <- as.character()
for (i in 1:length(TED$views_details)) {
  views_temp <- TED$views_details[i]
  views_temp <- strsplit(views_temp, "views")
  views_temp <- views_temp[[1]][1]
  views_time <- append(views_time,views_temp)
}
views_time <- gsub(" ","",views_time)
views_time <- gsub(",","",views_time)
TED$views_details <- as.numeric(views_time)

# Clean transcript
TED$tanscript <- gsub("^.+?00:(.*)","\\1",TED$tanscript)
TED$tanscript <- gsub("\r\n"," ",TED$tanscript)
TED$tanscript <- gsub("[[:digit:]]"," ",TED$tanscript)

# extract on version of TED for sentiment part
TED_sentiment <- TED
# no need for title column in the following analysis
TED <- TED %>% select(-title)
```

We also converted the topic names, AI, Climate change, and Relationships, to be numbers 1, 2, and 3 under **cate** variable in **TED**, respectively. This would be easy to keep track of the videos in supervised and unsupervised learning analyses.

Due to the limited number of available videos within the selected topics on TED website, we could not scrape more videos for unsupervised and supervised learning analyses, and we would like to obtain a robust model as well as avoid the overfitting problem. Therefore, we decided to increase the number of our observations by setting up a window of 20 sentences to be equal to 1 observation as we noticed that the transcripts each video comprised of more than 20 sentences. 

We split sentences by using the **tokenize_sentence** function from quanteda package and created a new variable, namely **sub cate**. For example, sub_cate of 1.1 indicates that the observation is from the first transcript in AI (The first topic). We then created a **text** variable to indicate and identify each text. For example, a text of 1.1.1 indicates that this text comes from the first 20 sentences of the first transcript in AI topic. By doing this, the number of observations increases from 286 to 1,471 and we named this data frame as **TED_full**. To sum up, **TED_full** consists of 1,471 observations with 7 variables which are posted, cate, like, view, subcate, text, tanscript. 

```{r, echo=TRUE, warning=FALSE}
# Increase the number of instances: 20 sentences = 1 instance
TED_full <- TED[0,]
TED_full$subcate <- TED_full$cate #new col but same type
TED_full$text <- TED_full$cate
n_transcript <- length(TED$tanscript)

sub_cate_1 = 0
sub_cate_2 = 0
sub_cate_3 = 0

for (i in 1:n_transcript) {
  
  if (TED$cate[i] == "1") {
    sub_cate_1 <- sub_cate_1 + 1
    subcat_temp =  paste(TED$cate[i],".",as.character(sub_cate_1), sep = "", collapse = "")
  } 
  else if (TED$cate[i] == "2") {
    sub_cate_2 <- sub_cate_2 + 1
    subcat_temp =  paste(TED$cate[i],".",as.character(sub_cate_2), sep = "", collapse = "")
  }
  else {
    sub_cate_3 <- sub_cate_3 + 1
    subcat_temp =  paste(TED$cate[i],".",as.character(sub_cate_3), sep = "", collapse = "")
  }
   
  transcript_i <- TED$tanscript[i]
  transcript_i_sentence <- unlist(tokenize_sentence(transcript_i))
  n_sen <- length(transcript_i_sentence)
  n_group <- ceiling(n_sen/20)
  for (j in 1:n_group) {
    if (j == n_group) {
      sentence_temp <- paste(transcript_i_sentence[((j-1)*20+1):(n_sen)], collapse = " ")
    } 
    else {
      sentence_temp <- paste(transcript_i_sentence[((j-1)*20+1):(j*20)], collapse = " ")
    }
    
    text_temp = paste(subcat_temp,".",as.character(j), sep = "", collapse = "")
    TED_temp <- data.frame(posted = TED$posted[i], cate = TED$cate[i], like = TED$likes[i], view = TED$views_details[i], subcate = subcat_temp, text = text_temp, tanscript = sentence_temp)
    TED_full <- rbind(TED_full, TED_temp)
  }
    
}

TED_full$tanscript <- trim_ws(TED_full$tanscript)

# Our final table = TED_full consisting of 1471 instances

kable(TED_full[10,], caption = "The example of  TED_full table") %>%
  kable_styling(bootstrap_options = "bordered") %>%
  kableExtra::scroll_box(width = "100%", height = "250px")

```

## Tokenization

We tokenized our transcript by quanteda package aiming to receive Document-Term Matrix and TFIDF matrix. In this section, we performed the tokenization twice. First, we tokenized **TED**, which consists of 286 videos/observations, to gain access into hidden insights each video and to observe the similarity and dissimilarity of each video. Second, we tokenized **TED_full**, which consists of 1,471 instances, for unsupervised and supervised learning analyses.

### Tokenization from TED 

We applied corpus() and tokens() functions to the tanscript variable to remove numbers, all characters in the "punctuation", symbols, and separators. We then removed stop words from the SMART information retrieval system in English (571 words) and also deleted 2 more words, applaud and laughter, that they appear often in our transcript as sound representation. Sound representation in a transcript is one of the translated functionality of TED meant to enable deaf and hard-of-hearing viewers to understand all the non-spoken auditory information. Afterward, we performed lemmatization and named the data frame as **TED.tk1**.

To obtain the Document-Term Matrix and the TFIDF matrix, we used dfm() and dfm_tfidf() functions, respectively. The first 10 terms and 10 documents (videos) are shown below.

```{r, echo=TRUE, warning=FALSE}
# Quanteda
TED.cp1 <- corpus(TED$tanscript)
#summary(TED.cp1)

TED.tk1 <- tokens(
  TED.cp1, 
  remove_numbers = TRUE, 
  remove_punct = TRUE, 
  remove_symbols = TRUE, 
  remove_separators = TRUE)

TED.tk1 <- TED.tk1 %>% 
  tokens_tolower() %>% 
  tokens_remove(c(stopwords(source = "smart"), "applaud", "laughter"))

TED.tk1 <- tokens_replace(
  TED.tk1,
  pattern = hash_lemmas$token, 
  replacement = hash_lemmas$lemma)

TED.dfm1 <- dfm(TED.tk1)
kable(TED.dfm1[1:10,1:10], caption = "The example of Document-Term Matrix") %>%
  kable_styling(bootstrap_options = "bordered") %>%
  kableExtra::scroll_box(width = "100%", height = "250px")

TED.tfidf1 <- dfm_tfidf(TED.dfm1)  
kable(TED.tfidf1[1:10,1:10], caption = "The example of TFIDF matrix") %>%
  kable_styling(bootstrap_options = "bordered") %>%
  kableExtra::scroll_box(width = "100%", height = "250px")

```

Additionally, the top 10 most frequent terms can simply be obtained using textstat_frequency() as presented in the last table. We notice that 

```{r, echo=TRUE, warning=FALSE}
TED.freq1 <- textstat_frequency(TED.dfm1)
head(TED.freq1, 10)

ggplot(TED.freq1,
       aes(x = rank, y = frequency, label = feature)) + 
  geom_point() + 
  geom_text_repel() +
  ggtitle("The relationships of frequency and rank")

```

We can observe some deviation from Zipf's law on the right hand side of the chart (low frequency words). The reason of the deviation might be that the text we are analyzing is not a representative sample of the language. For example, the text might contain a lot of technical terms.

```{r, echo=TRUE, warning=FALSE}
ggplot(TED.freq1,
       aes(x = rank, y = frequency, label = feature)) + 
  geom_point() + 
  geom_text_repel() +
  scale_x_log10() +
  scale_y_log10() +
  ggtitle("The relationships of frequency and rank on log10-log10 scale") 

```


### Tokenization from TED_full for unsupervised and supervised learning analyses



I aim to create DTM, TFIDF tables by quanteda package. 
```{r, echo=TRUE, warning=FALSE}
# Quanteda
TED.cp <- corpus(TED_full$tanscript)
summary(TED.cp)

TED.tk <- tokens(
  TED.cp, 
  remove_numbers = TRUE, 
  remove_punct = TRUE, 
  remove_symbols = TRUE, 
  remove_separators = TRUE)

TED.tk <- TED.tk %>% 
  tokens_tolower() %>% 
  tokens_remove(c(stopwords(source = "smart"), "applaud", "laughter"))

TED.tk <- tokens_replace(
  TED.tk,
  pattern = hash_lemmas$token, 
  replacement = hash_lemmas$lemma)

TED.tk

```

## DTM

```{r, echo=TRUE, warning=FALSE}
TED.dfm <- dfm(TED.tk)
head(TED.dfm)
```

```{r, echo=TRUE, warning=FALSE}
TED.freq <- textstat_frequency(TED.dfm)
head(TED.freq, 20)
```

## TF-IDF

```{r, echo=TRUE, warning=FALSE}
# Quenteda
TED.tfidf <- dfm_tfidf(TED.dfm)  
sort(apply(TED.tfidf, 2, max), decreasing = TRUE)[1:10]

```





