---
title: "Topic modeling"
author: "Ting Yang"
date: "05/12/2022"
output: html_document
---

# LSA

## LSA on TF
First, we build the LSA object and use 4 dimensions. Latent Semantic Analysis(LSA) decomposes this DTM(TED.dfm) into 3 matrices ($M = U\Sigma V^{t}$), centred around 4 topics. We check the 3 matrices: *U:Doc-topic sim*, *Î£:Topic strength* and *V:Terms-topic sim*. 
```{r}
TED.lsa <- textmodel_lsa(x = TED.dfm,nd = 4)
```

```{r Doc-topic sim}
head(TED.lsa$docs)
```
This Doc-topic sim table shows the link between each text and each topic. For example, text1 most relevant to dimension 2(topic 2).

```{r}
head(TED.lsa$sk)
```
These values represent the strength of each topic. Except for the topic 1, topic 2 has the largest strength.


```{r}
head(TED.lsa$features)
```
This Terms-topic sim table shows the link between each term and each topic. For example, the term "artificial" most relevant to dimension 2(topic 2).


The first dimension of LSA is often correlated to (of little information and often not represented) the document length and the frequency of the term. We build a scatter-plot between the document length and Dimension 1 to demonstrate it.
```{r}
doc.freq <- ntoken(TED.tk) # row-sum of the DTM.
data.frame(doc.freq,
           dim1 = TED.lsa$docs[, 1]) %>% 
  ggplot(aes(doc.freq, dim1)) + 
  geom_point() + 
  geom_smooth(method="lm") +
  xlab("Number of tokens") + 
  ylab("LSA dim. 1")
```

Then we check the top words for dimension2, 3, and 4.
```{r}
n.terms <- 5
## For Dimension 2
w.order <- sort(TED.lsa$features[, 2],decreasing = TRUE)
w.top.d2 <- c(w.order[1:n.terms],rev(rev(w.order)[1:n.terms]))
## For Dimension 3
w.order <- sort(TED.lsa$features[, 3], decreasing = TRUE)
w.top.d3 <- c(w.order[1:n.terms], rev(rev(w.order)[1:n.terms]))
## For Dimension 4
w.order <- sort(TED.lsa$features[,4], decreasing = TRUE)
w.top.d4 <- c(w.order[1:n.terms], rev(rev(w.order)[1:n.terms]))
```


```{r}
w.top.d2
```
Dimension 2 is associated positively with word like "ai", "human", "robot", "machine" ,"datum", and negatively associated with "feel", "climate", "life", "love", "people".


```{r}
w.top.d3
```
Dimension 3 is associated positively with word like "people", "love", "robot", "fell" ,"life", and negatively associated with "forest", "year", "energy", "carbon", "climate".


```{r}
w.top.d4
```
Dimension 4 is associated positively with word like "robot", "thing", "rule", "move" ,"start", and negatively associated with "datum", "human", "love", "people", "ai".


In order to check the relation between LSA and category of text, we combine the LSA result with the category of document and represent every text on these two following plots. 
```{r}

TED.lsa.source <- TED_full %>% 
  select(2) %>% cbind(as.data.frame(TED.lsa$docs))

ggplot(data=TED.lsa.source,mapping = aes(
  x=V2,
  y=V3,
  color=cate))+
  geom_point()+
  labs(x = "dimension2",
       y = "dimension3",
       title = "Distribution of texts in different category on the LSA(TF) dimension 2 and 3")+
  scale_colour_discrete(
    name="Category",
    breaks=c("1","2","3"),
    labels=c("AI","Climate change","Relationships")
  )
```
First plot:x-axis is dimension 2 and y-axis is dimension3.
According to this plot, most of the texts of category "Climate change" are negatively associated with dimension3. Most of the texts of category "Relationships" are positively associated with dimension3. And most of the category "AI" are positively associated with dimension2.

```{r}
ggplot(data=TED.lsa.source,mapping = aes(
  x=V3,
  y=V4,
  color=cate))+
  geom_point()+
  labs(x = "dimension3",
       y = "dimension4",
       title = "Distribution of texts in different category on the LSA(TF) dimension 3 and 4")+
  scale_colour_discrete(
    name="Category",
    breaks=c("1","2","3"),
    labels=c("AI","Climate change","Relationships")
  )
```
Second plot:x-axis is dimension 3 and y-axis is dimension4.
According to this plot, most texts of category "AI" are positively associated with dimension4, most of the texts of category "Climate change" are negatively associated with dimension3, and most of the texts of category "Relationships" are positively associated with dimension3.


## LSA on TF-IDF
Repeat the LSA with the TF-IDF as DTM. Check whether the weighted frequency can make the LSA results better interpret texts.
```{r}
TED.lsa2 <- textmodel_lsa(TED.tfidf, nd = 4) 
```


```{r}
head(TED.lsa2$docs)
```


```{r}
head(TED.lsa2$sk)
```

```{r}
head(TED.lsa2$features)
```

We also check the top words for dimension2, 3, and 4 of LSA on TF-IDF.
```{r}
## For Dimension 2
w2.order <- sort(TED.lsa2$features[, 2],decreasing = TRUE)
w2.top.d2 <- c(w2.order[1:n.terms],rev(rev(w2.order)[1:n.terms]))
## For Dimension 3
w2.order <- sort(TED.lsa2$features[, 3], decreasing = TRUE)
w2.top.d3 <- c(w2.order[1:n.terms], rev(rev(w2.order)[1:n.terms]))
## For Dimension 4
w2.order <- sort(TED.lsa2$features[, 4], decreasing = TRUE)
w2.top.d4 <- c(w2.order[1:n.terms], rev(rev(w2.order)[1:n.terms]))
```


```{r}
w2.top.d2
```
For this LSA, dimension 2 is associated positively with word like "forest", "carbon", "climate", "emission" ,"energy", and negatively associated with "human", "computer", "machine", "ai", "robot".

```{r}
w2.top.d3
```
Dimension 3 is associated positively with word like "regret", "sex", "woman", "love" ,"man", and negatively associated with "datum", "machine", "rule", "ai", "robot".

```{r}
w2.top.d4
```
Dimension 4 is associated positively with word like "robot", "rule", "bee", "seaweed" ,"coral", and negatively associated with "machine", "human", "company", "datum", "ai".


We also check the relation between this LSA result and category of text, we combine the LSA result with the category of document and represent every text on these two following plots. 
```{r}
TED.lsa2.source <- TED_full %>% 
  select(2) %>% cbind(as.data.frame(TED.lsa2$docs))

ggplot(data=TED.lsa2.source,mapping = aes(
  x=V2,
  y=V3,
  color=cate))+
  geom_point()+
  labs(x = "dimension2",
       y = "dimension3",
       title = "Distribution of texts in different category on the LSA(TF-IDF) dimension 2 and 3")+
  scale_colour_discrete(
    name="Category",
    breaks=c("1","2","3"),
    labels=c("AI","Climate change","Relationships")
  )
```
First plot:x-axis is dimension 2 and y-axis is dimension3.
According to this plot, most of the texts of category "Climate change" are positively associated with dimension2. Most of the texts of category "Relationships" are positively associated with dimension3. And most of the category "AI" are negatively associated with dimension2 and dimension3.

```{r}
ggplot(data=TED.lsa2.source,mapping = aes(
  x=V3,
  y=V4,
  color=cate))+
  geom_point()+
  labs(x = "dimension3",
       y = "dimension4",
       title = "Distribution of texts in different category on the LSA(TF-IDF) dimension 3 and 4")+
  scale_colour_discrete(
    name="Category",
    breaks=c("1","2","3"),
    labels=c("AI","Climate change","Relationships")
  )
```
Second plot:x-axis is dimension 3 and y-axis is dimension4.
According to this plot, most texts of category "AI" are associated with dimension4. But part of texts of category "AI" are negatively associated with dimension 4 and part of texts of category "AI" are positively associated with dimension 4. The pattern is not very clear.


# LDA
We now turn to Latent Dirichlet Association (LDA). LDA is a Bayesian model for topic modeling: generative model. It is also to discover topics in a collection of documents. For the illustration, we will make 4 topics again.
```{r}
TED.LDA <- LDA(
  convert(TED.dfm, to = "topicmodels"),
  k = 4,
  control = list(seed = 123))
```

First, I check the top5 words in each dimension. For example, the top5 terms for topic 1 are "climate", "year", "make", "change" and "energy".
```{r}
topicmodels::terms(TED.LDA, 5)
```

Then I create a table to show the number of documents in each dimension. For example, topic 3 has the highest number of documents(439). 
```{r}
topicmodels::topics(TED.LDA)%>% table()
```

Then I use the topic_diagnostics function to diagnose the *prominence*, *coherence* and *exclusivity* of each dimension.
```{r}
topic_diagnostics(
  topic_model = TED.LDA, 
  dtm_data = convert(TED.dfm, to = "topicmodels"))
```
Topic 3 has the largest prominence.
Topic 4 has the largest topic coherence and Topic 1 has the smallest topic coherence.
Topic 1 has the largest topic exclusivity and Topic 4 has the smallest topic exclusivity.


```{r}
beta.long <- tidy(
  TED.LDA,
  matrix = "beta") # equivalent to melt (with this package)

beta.long %>% 
  group_by(topic) %>% 
  top_n(15, beta) %>% 
  ggplot(aes(reorder_within(term, beta, topic), beta)) + 
  geom_col(show.legend = FALSE) +
  coord_flip()+
  facet_wrap(~ topic, scales = "free_y") +
  scale_x_reordered() + 
  xlab("Term") +
  theme(
    axis.text.y = element_text(size = 8),
    axis.text.x = element_text(size = 8),
    strip.text = element_text(size = 8))
```
Topic1 focus on terms like "climate", "change", "energy", "water". 
Topic2 focus on terms like"people", "ai", "work", "technology". 
Topic3 focus on terms like"love", "life", "woman", "relationship". 
Topic4 focus on terms like "robot", "thing", "brain", "human". 

```{r}

document <- rownames(TED.lsa.source)
TED.lsa.source <- cbind(document,TED.lsa.source)

gamma.long <- tidy(TED.LDA,matrix = "gamma") %>% 
  right_join(TED.lsa.source[1:2],by = "document")

gamma.long$cate<-factor(gamma.long$cate,
                       levels = c('1','2','3'),
                       labels = c("AI","Climate change","Relationships"))

gamma.long %>% ggplot(mapping = aes(x=document,y=gamma,fill=cate))+
  geom_col()+
  coord_flip() + 
  facet_wrap(~topic,ncol = 4)
  
  
```

The *Climate change* related documents mainly talk about Topic 1. 
The *Relationships* related documents mainly talk about Topic3. 
The *AI* related documents mainly talk about Topic 2 and Topic4. 

