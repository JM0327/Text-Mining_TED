---
title: "Topic modeling"
author: "Ting Yang"
date: "05/12/2022"
output: html_document
---

# LSA

## LSA on TF
First, we build the LSA object and use 4 dimensions.And we check the *Doc-topic sim*, *Topic strength* and *Terms-topic sim*.
```{r}
TED.lsa <- textmodel_lsa(x = TED.dfm,nd = 4)
```

```{r}
head(TED.lsa$docs)
```
This table shows the link between each text and each topic. For example, text1 most relevant to dimension 2(topic 2).

```{r}
head(TED.lsa$sk)
```
These values represent the strength of each topic. Except for the topic 1, topic 2 has the largest strength.


```{r}
head(TED.lsa$features)
```
This table shows the link between each term and each topic. For example, the term "artificial" most relevant to dimension 2(topic 2).


The first dimension is often correlated to (of little information and often not represented.) the document length and the frequency of the term. So we check the top words for dimension2, 3, and 4.
```{r}
n.terms <- 5
## For Dimension 2
w.order <- sort(TED.lsa$features[, 2],decreasing = TRUE)
w.top.d2 <- c(w.order[1:n.terms],rev(rev(w.order)[1:n.terms]))
## For Dimension 3
w.order <- sort(TED.lsa$features[, 3], decreasing = TRUE)
w.top.d3 <- c(w.order[1:n.terms], rev(rev(w.order)[1:n.terms]))
## For Dimension 4
w.order <- sort(TED.lsa$features[,4], decreasing = TRUE)
w.top.d4 <- c(w.order[1:n.terms], rev(rev(w.order)[1:n.terms]))
```

```{r}
w.top.d2
```
Dimension 2 is associated positively with word like "ai", "human", "robot", "machine" ,"datum", and negatively associated with "feel", "climate", "life", "love", "people".


```{r}
w.top.d3
```
Dimension 3 is associated positively with word like "people", "love", "robot", "fell" ,"life", and negatively associated with "forest", "year", "energy", "carbon", "climate".


```{r}
w.top.d4
```
Dimension 4 is associated positively with word like "robot", "thing", "rule", "move" ,"start", and negatively associated with "datum", "human", "love", "people", "ai".


In order to check the relation between LSA and category of text, we combine the LSA result with the category of document and represent every text on these two following plots. 
```{r}

TED.lsa.source <- TED_full %>% 
  select(2) %>% cbind(as.data.frame(TED.lsa$docs))

ggplot(data=TED.lsa.source,mapping = aes(
  x=V2,
  y=V3,
  color=cate))+
  geom_point()+
  labs(x = "dimension2",
    y = "dimension3")+
  scale_colour_discrete(
    name="Category",
    breaks=c("1","2","3"),
    labels=c("AI","Climate change","Relationships")
  )
```
First plot:x-axis is dimension 2 and y-axis is dimension3.
According to this plot, most of the texts of category "Climate change" are negatively associated with dimension3. Most of the texts of category "Relationships" are positively associated with dimension3. And most of the category "AI" are positively associated with dimension2.

```{r}
ggplot(data=TED.lsa.source,mapping = aes(
  x=V3,
  y=V4,
  color=cate))+
  geom_point()+
  labs(x = "dimension3",
    y = "dimension4")+
  scale_colour_discrete(
    name="Category",
    breaks=c("1","2","3"),
    labels=c("AI","Climate change","Relationships")
  )
```
Second plot:x-axis is dimension 3 and y-axis is dimension4.
According to this plot, most texts of category "AI" are positively associated with dimension4.

## LSA on TF-IDF
Repeat the LSA with the TF-IDF as DTM. Check whether the weighted frequency can make the LSA results better interpret texts.
```{r}
TED.lsa2 <- textmodel_lsa(TED.tfidf, nd = 4) 
```


```{r}
head(TED.lsa2$docs)
```


```{r}
head(TED.lsa2$sk)
```

```{r}
head(TED.lsa2$features)
```
We also check the top words for dimension2, 3, and 4 of LSA on TF-IDF.
```{r}
## For Dimension 2
w2.order <- sort(TED.lsa2$features[, 2],decreasing = TRUE)
w2.top.d2 <- c(w2.order[1:n.terms],rev(rev(w2.order)[1:n.terms]))
## For Dimension 3
w2.order <- sort(TED.lsa2$features[, 3], decreasing = TRUE)
w2.top.d3 <- c(w2.order[1:n.terms], rev(rev(w2.order)[1:n.terms]))
## For Dimension 4
w2.order <- sort(TED.lsa2$features[, 4], decreasing = TRUE)
w2.top.d4 <- c(w2.order[1:n.terms], rev(rev(w2.order)[1:n.terms]))
```

```{r}
w2.top.d2
```
For this LSA, dimension 2 is associated positively with word like "forest", "carbon", "climate", "emission" ,"energy", and negatively associated with "human", "computer", "machine", "ai", "robot".

```{r}
w2.top.d3
```
Dimension 3 is associated positively with word like "regret", "sex", "woman", "love" ,"man", and negatively associated with "datum", "machine", "rule", "ai", "robot".

```{r}
w2.top.d4
```
Dimension 4 is associated positively with word like "robot", "rule", "bee", "seaweed" ,"coral", and negatively associated with "machine", "human", "company", "datum", "ai".


We also check the relation between this LSA result and category of text, we combine the LSA result with the category of document and represent every text on these two following plots. 
```{r}
TED.lsa2.source <- TED_full %>% 
  select(2) %>% cbind(as.data.frame(TED.lsa2$docs))

ggplot(data=TED.lsa2.source,mapping = aes(
  x=V2,
  y=V3,
  color=cate))+
  geom_point()+
  labs(x = "dimension2",
    y = "dimension3")+
  scale_colour_discrete(
    name="Category",
    breaks=c("1","2","3"),
    labels=c("AI","Climate change","Relationships")
  )
```
First plot:x-axis is dimension 2 and y-axis is dimension3.
According to this plot, most of the texts of category "Climate change" are positively associated with dimension2. Most of the texts of category "Relationships" are positively associated with dimension3. And most of the category "AI" are negatively associated with dimension2 and dimension3.

```{r}
ggplot(data=TED.lsa2.source,mapping = aes(
  x=V2,
  y=V4,
  color=cate))+
  geom_point()+
  labs(x = "dimension2",
    y = "dimension4")+
  scale_colour_discrete(
    name="Category",
    breaks=c("1","2","3"),
    labels=c("AI","Climate change","Relationships")
  )
```
Second plot:x-axis is dimension 3 and y-axis is dimension4.
According to this plot, most texts of category "AI" are positively associated with dimension4. Part of texts of category "AI" are negatively associated with dimension 4 and part of texts of category "AI" are positively associated with dimension 4.


# LDA
We now turn to Latent Dirichlet Association (LDA). For the illustration, we will make 4 topics again.
```{r}
TED.LDA <- LDA(
  convert(TED.dfm, to = "topicmodels"),
  k = 4)
```

First, I check the top5 words in each dimensions.
```{r}
topicmodels::terms(TED.LDA, 5)
```

Then I create a table to show the number of documents in each dimension.
```{r}
topicmodels::topics(TED.LDA)%>% table()
```

Then I use the topic_diagnostics function to diagnose the *prominence*, *coherence* and *exclusivity* of each dimension.
```{r}
topic_diagnostics(
  topic_model = TED.LDA, 
  dtm_data = convert(TED.dfm, to = "topicmodels"))
```
Topic 4 has the largest prominence.
Topic 1 has the largest topic coherence and Topic 2 has the smallest topic coherence.
Topic 2 has the largest topic exclusivity and Topic 1 has the smallest topic exclusivity.


```{r}
beta.long <- tidy(
  TED.LDA,
  matrix = "beta") # equivalent to melt (with this package)

beta.long %>% 
  group_by(topic) %>% 
  top_n(15, beta) %>% 
  ggplot(aes(reorder_within(term, beta, topic), beta)) + 
  geom_col(show.legend = FALSE) +
  coord_flip()+
  facet_wrap(~ topic, scales = "free_y") +
  scale_x_reordered() + 
  xlab("Term") +
  theme(
    axis.text.y = element_text(size = 8),
    axis.text.x = element_text(size = 8),
    strip.text = element_text(size = 8))
```
Topic1 focus on terms like "people", "work", "make", "thing". 
Topic2 focus on terms like"year", "climate", "water", "energy". 
Topic3 focus on terms like"love", "life", "feel", "time". 
Topic4 focus on terms like "human", "ai", "robot", "make". 

```{r}

document <- rownames(TED.lsa.source)
TED.lsa.source <- cbind(document,TED.lsa.source)

gamma.long <- tidy(TED.LDA,matrix = "gamma") %>% 
  right_join(TED.lsa.source[1:2],by = "document")

gamma.long$cate<-factor(gamma.long$cate,
                       levels = c('1','2','3'),
                       labels = c("AI","Climate change","Relationships"))

gamma.long %>% ggplot(mapping = aes(x=document,y=gamma,fill=cate))+
  geom_col()+
  coord_flip() + 
  facet_wrap(~topic,ncol = 4)
  
  
```

The *Climate change* related documents mainly talk about Topic 2. 
The *Relationships* related documents mainly talk about Topic3. 
The *AI* related documents mainly talk about Topic4. 

# Supervised learning(LSA on TF)
First, we use LSA on TF to have a reduced dimension version of the DFM and build a random forest model to predict the category from the LSA.

We build our data frame consisting of the category and the “doc” matrix of the LSA. Along with this, we build the training set index based on a 80/20 split.
```{r}
a <- c(1:1471)
row.names(TED.lsa.source) <- a
TED.lsa.source$cate <- as.factor(TED.lsa.source$cate) 
```


```{r}
set.seed(123)
index.tr <- createDataPartition(y = TED.lsa.source$cate, p= 0.8, list = FALSE)
TED.tr <- TED.lsa.source[index.tr,]
TED.te <- TED.lsa.source[-index.tr,]
```

## Balance the data
The data is unbalanced so We need to use the sub-sampling method to balance the data.
```{r}
table(TED.tr$cate)
```


```{r}
set.seed(123)
n2 <- min(table(TED.tr$cate)) ## 327

TED.tr.1 <- filter(TED.tr, cate=="1") ## the category 1
TED.tr.2 <- filter(TED.tr, cate=="2") ## the category 2
TED.tr.3 <- filter(TED.tr, cate=="3") ## the category 3
index.1 <- sample(size=n2, x=1:nrow(TED.tr.1), replace=FALSE)
index.3 <- sample(size=n2, x=1:nrow(TED.tr.3), replace=FALSE)
TED.tr.subs <- data.frame(rbind(TED.tr.1[index.1,], 
                                TED.tr.2,
                                TED.tr.3[index.3,]))
```


```{r}
table(TED.tr.subs$cate)
```
We now use a random forest to predict the category from the LSA on TF. The resulting accuracy is inspected on the test set.
```{r}
TED.fit <- ranger(TED.tr.subs$cate ~ ., 
                     data = TED.tr.subs[2:6])
pred.te <- predict(TED.fit, TED.te)
confusionMatrix(data=pred.te$predictions, reference = TED.te$cate)
```
According to the confusion matrix, the accuracy is *0.8089* and the balanced accuracy for class 1 is *0.8340*, for class 2 is *0.8911*, and for class 3 is *0.8576*.

# Supervised learning(LSA on TF-IDF)
Now we repeat the steps and use LSA on TF-IDF to have a reduced dimension version of the DFM and build a random forest model to predict the category from the LSA.
```{r}
TED.lsa2.source <- cbind(document,TED.lsa2.source)
row.names(TED.lsa2.source) <- a
TED.lsa2.source$cate <- as.factor(TED.lsa2.source$cate)
```

```{r}
set.seed(123)
index.tr <- createDataPartition(y = TED.lsa2.source$cate, p= 0.8, list = FALSE)
TED.tr <- TED.lsa2.source[index.tr,]
TED.te <- TED.lsa2.source[-index.tr,]
```

```{r}
n2 <- min(table(TED.tr$cate)) ## 327

TED.tr.1 <- filter(TED.tr, cate=="1") ## the category 1
TED.tr.2 <- filter(TED.tr, cate=="2") ## the category 2
TED.tr.3 <- filter(TED.tr, cate=="3") ## the category 3
index.1 <- sample(size=n2, x=1:nrow(TED.tr.1), replace=FALSE)
index.3 <- sample(size=n2, x=1:nrow(TED.tr.3), replace=FALSE)
TED.tr.subs <- data.frame(rbind(TED.tr.1[index.1,], 
                                TED.tr.2,
                                TED.tr.3[index.3,]))
```


```{r}
TED.fit2 <- ranger(TED.tr.subs$cate ~ ., 
                     data = TED.tr.subs[2:6])
pred.te <- predict(TED.fit2, TED.te)
confusionMatrix(data=pred.te$predictions, reference = TED.te$cate)
```
According to the confusion matrix, the model build on features using LSA on TF-ITF is better than the model build on features using LSA on TF. The accuracy is *0.8805* and the balanced accuracy for class 1 is *0.8897*, for class 2 is *0.9317*, and for class 3 is *0.9156*.
