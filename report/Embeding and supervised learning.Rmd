---
title: "Embedding"
author: "Ting Yang"
date: "09/12/2022"
output: html_document
---

# Word Embedding

```{r}
TED.coo <- fcm(TED.tk,
               context = "window",
               window = 5,
               tri = FALSE) 
head(TED.coo)
```

```{r}
set.seed(123)
p <- 2 # word embedding dimension
TED.glove <- GlobalVectors$new(rank = p,
                               x_max = 10) # x_max is a needed technical option
TED.we <- TED.glove$fit_transform(TED.coo) # central vectors; speech.glove$components contains the context vectors
TED.we <- t(TED.glove$components) + TED.we# unique representation
```

```{r}
index <- textstat_frequency(dfm(TED.tk))[1:150, ]$feature
index ## words with the 50 largest frequencies
```

```{r}
data.for.plot <- data.frame(TED.we[index, ])
data.for.plot$word <- row.names(data.for.plot)
ggplot(data.for.plot, 
       aes(x = X1,
           y = X2,
           label = word)) +
  geom_text_repel(max.overlaps = 100) + 
  theme_void() 
```
Words that are close on the map are often used together (e.g. “ai”, “robot”, “problem”).

# Document Embedding

```{r}
head(TED.tk[[1]])
head(TED.we[TED.tk[[1]], ])
apply(TED.we[TED.tk[[1]],], 2, mean) ## average all these vectors => Document 1 vector
```

```{r}
nd <- length(TED.tk) # number of documents
TED.de <- matrix(nr = nd, nc = p) # document embedding matrix (1 document per row)
for(i in 1:nd) {
  words_in_i <- TED.we[TED.tk[[i]], , drop = FALSE] 
  # drop = FALSE is needed in case there is only one token
  TED.de[i, ] <- apply(words_in_i, 2 ,mean)
}
row.names(TED.de) <- names(TED.tk)
head(TED.de) ## document vectors
```

```{r}
TED.de <- as.data.frame(TED.de)
TED.de.source <- TED_full %>% 
  select(2) %>% cbind(TED.de)

ggplot(data=TED.de.source,mapping = aes(
  x=TED.de.source$V1,
  y=TED.de.source$V2,
  color=TED.de.source$cate))+
  geom_point()+
  labs(x = "dimension1",
    y = "dimension2")+
  scale_colour_discrete(
    name="Category",
    breaks=c("1","2","3"),
    labels=c("AI","Climate change","Relationships")
  )
```


# Supervised learning
```{r}
row.names(TED.de.source) <- a
TED.de.source$cate <- as.factor(TED.de.source$cate) 
```

```{r}
set.seed(123)

TED.tr <- TED.de.source[index.tr,]
TED.te <- TED.de.source[-index.tr,]
```

```{r}
table(TED.tr$cate)
```
```{r}
n2 <- min(table(TED.tr$cate)) ## 327

TED.tr.1 <- filter(TED.tr, cate=="1") ## the category 1
TED.tr.2 <- filter(TED.tr, cate=="2") ## the category 2
TED.tr.3 <- filter(TED.tr, cate=="3") ## the category 3
index.1 <- sample(size=n2, x=1:nrow(TED.tr.1), replace=FALSE)
index.3 <- sample(size=n2, x=1:nrow(TED.tr.3), replace=FALSE)
TED.tr.subs <- data.frame(rbind(TED.tr.1[index.1,], 
                                TED.tr.2,
                                TED.tr.3[index.3,]))
```

```{r}
TED.fit <- ranger(TED.tr.subs$cate~., 
                     data = TED.tr.subs)
pred.te <- predict(TED.fit, TED.te)
confusionMatrix(data=pred.te$predictions, reference = TED.te$cate)
```

# Combine LSA and embeding for supervised learning

```{r}
TED.de.source <- TED.de.source %>% rename(V5=V1,V6=V2)
TED.LSA.Emb <- TED.lsa.source %>% cbind(TED.de.source) %>% 
  select(-7)
```


```{r}
set.seed(123)

TED.tr <- TED.LSA.Emb[index.tr,]
TED.te <- TED.LSA.Emb[-index.tr,]
```

```{r}
TED.tr.1 <- filter(TED.tr, cate=="1") ## the category 1
TED.tr.2 <- filter(TED.tr, cate=="2") ## the category 2
TED.tr.3 <- filter(TED.tr, cate=="3") ## the category 3
index.1 <- sample(size=n2, x=1:nrow(TED.tr.1), replace=FALSE)
index.3 <- sample(size=n2, x=1:nrow(TED.tr.3), replace=FALSE)
TED.tr.subs <- data.frame(rbind(TED.tr.1[index.1,], 
                                TED.tr.2,
                                TED.tr.3[index.3,]))
```


```{r}
TED.fit <- ranger(TED.tr.subs$cate~., 
                     data = TED.tr.subs[2:8])
pred.te <- predict(TED.fit, TED.te)
confusionMatrix(data=pred.te$predictions, reference = TED.te$cate)
```

